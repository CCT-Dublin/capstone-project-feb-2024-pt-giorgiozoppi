{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>TrackML/HotelRank</b>: Elevating Revenue Performance Through Machine Learning and Deep Learning Techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revenue management is a very important to make profits in the hotel industry, three main factors play an important role to get it right:  \n",
    "    - Hotel room demand over time (demand forecast)\n",
    "    - Prediction of booking cancellations.\n",
    "    - Online hotel reputation.\n",
    "In this project we take in account each one thru a linear combination of different scores that represent each item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>DemandScore</b>: Demand Forecast over 12 months booking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective to section to concentrate ourselves to demand forecast with just Blastness dataset and nothing more:\n",
    "- We have to clean the data provided and see patterns.\n",
    "- Detect outliers in the demand\n",
    "- Compare SARIMAX and prophet neural network to see which fits better for the demandscore.\n",
    "- Create the demand score for each hotel using booking forecast in next temporal year frame.\n",
    "- We leave to future work any crossdata about the demand related to weather and external events.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bamboolib in /home/jozoppi/anaconda3/lib/python3.12/site-packages (1.30.19)\n",
      "Requirement already satisfied: attrs>=20.3.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (23.1.0)\n",
      "Requirement already satisfied: cryptography>=2.6.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (42.0.5)\n",
      "Requirement already satisfied: ipywidgets<8.0.0,>=7.6.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (7.8.1)\n",
      "Requirement already satisfied: jedi<1.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (0.18.1)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.1.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (1.5.3)\n",
      "Requirement already satisfied: packaging>=19.2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (23.2)\n",
      "Requirement already satisfied: plotly<6.0.0,>=4.9.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (5.22.0)\n",
      "Requirement already satisfied: ppscore<2.0.0,>=1.2.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (1.3.0)\n",
      "Requirement already satisfied: psutil<6,>=5.4.2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (5.9.0)\n",
      "Requirement already satisfied: pygments in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (2.15.1)\n",
      "Requirement already satisfied: ipyslickgrid==0.0.3 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (0.0.3)\n",
      "Requirement already satisfied: scikit-learn<2.0.0,>=0.20.2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (1.4.2)\n",
      "Requirement already satisfied: statsmodels<1.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (0.14.2)\n",
      "Requirement already satisfied: toml>=0.10.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (0.10.2)\n",
      "Requirement already satisfied: xlrd>=1.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bamboolib) (2.0.1)\n",
      "Requirement already satisfied: notebook>=4.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipyslickgrid==0.0.3->bamboolib) (7.0.8)\n",
      "Requirement already satisfied: cffi>=1.12 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from cryptography>=2.6.1->bamboolib) (1.16.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.2.1)\n",
      "Requirement already satisfied: ipython-genutils~=0.2.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=3.6.6 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (3.6.6)\n",
      "Requirement already satisfied: ipython>=4.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (8.25.0)\n",
      "Requirement already satisfied: jupyterlab-widgets<3,>=1.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipywidgets<8.0.0,>=7.6.0->bamboolib) (1.0.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jedi<1.0.0->bamboolib) (0.8.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from pandas<2.0.0,>=1.1.0->bamboolib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from pandas<2.0.0,>=1.1.0->bamboolib) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from pandas<2.0.0,>=1.1.0->bamboolib) (1.26.4)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from plotly<6.0.0,>=4.9.0->bamboolib) (8.2.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from scikit-learn<2.0.0,>=0.20.2->bamboolib) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from scikit-learn<2.0.0,>=0.20.2->bamboolib) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from scikit-learn<2.0.0,>=0.20.2->bamboolib) (2.2.0)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from statsmodels<1.0.0->bamboolib) (0.5.6)\n",
      "Requirement already satisfied: pycparser in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.6.1->bamboolib) (2.21)\n",
      "Requirement already satisfied: decorator in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (3.0.43)\n",
      "Requirement already satisfied: stack-data in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (4.8.0)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.22.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.25.1)\n",
      "Requirement already satisfied: jupyterlab<4.1,>=4.0.2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (4.0.11)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (6.4.1)\n",
      "Requirement already satisfied: six in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from patsy>=0.5.6->statsmodels<1.0.0->bamboolib) (1.16.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (21.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (3.1.4)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (7.10.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (5.9.2)\n",
      "Requirement already satisfied: overrides>=5.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (7.4.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.14.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.0.4)\n",
      "Requirement already satisfied: ipykernel in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (6.28.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyterlab<4.1,>=4.0.2->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.2.0)\n",
      "Requirement already satisfied: babel>=2.10 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.32.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.2.5)\n",
      "Requirement already satisfied: executing in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=4.0.0->ipywidgets<8.0.0,>=7.6.0->bamboolib) (0.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (21.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.1.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.10.6)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (3.10.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.16.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.22.1->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2024.8.30)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.6.7)\n",
      "Requirement already satisfied: nest-asyncio in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from ipykernel->jupyterlab<4.1,>=4.0.2->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.6.0)\n",
      "Requirement already satisfied: webencodings in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (0.5.1)\n",
      "Requirement already satisfied: fqdn in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.1)\n",
      "Requirement already satisfied: uri-template in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (24.8.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (2.5)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /home/jozoppi/anaconda3/lib/python3.12/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook>=4.0.0->ipyslickgrid==0.0.3->bamboolib) (1.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install bamboolib\n",
    "!pip install pandas numpy seaborn matplot scikit-learn pyarrow prophet statsmodels pycaret datasets \n",
    "!python -m bamboolib install_nbextensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As first action we load the csv provided by Blastness and we create a dataset with columns names in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "filelist = glob.glob('./hoteldataset/*.csv')\n",
    "hotelsbookings = []\n",
    "column_mapping = {\n",
    "    'Codice': 'Code',\n",
    "    'Status': 'Status',\n",
    "    'Canale': 'BookingChannel',\n",
    "    'Arrivo': 'Arrival',\n",
    "    'Partenza': 'Departure',\n",
    "    'Notti': 'Nights',\n",
    "    'Totale': 'Total',\n",
    "    'Data acquisto': 'PurchaseDate',\n",
    "    'Dispositivo': 'BoookingDevice',\n",
    "    'Data Ultima Modifica/Cancellazione': 'LastModified'\n",
    "}\n",
    "for idx,f in enumerate(filelist):\n",
    "    df = pd.read_csv(f)\n",
    "    select_columns = list(column_mapping.keys())\n",
    "    current_df = df[select_columns]\n",
    "    remap = current_df.rename(columns=column_mapping)\n",
    "    hotel_id = \"\"\n",
    "    if idx < 9:\n",
    "        hotel_id=f'00{idx+1}'\n",
    "    else:\n",
    "        hotel_id=f'0{idx+1}'\n",
    "\n",
    "    remap['HotelId'] = hotel_id\n",
    "    hotelsbookings.append(remap)\n",
    "hotelsbookings[9].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge all hotels in a single dataframe and save to disk. We note that we need to divide cancelled and confirmed booking. Later since we want do forecast on the confirmed.\n",
    "- Also we need to categorize the origin\n",
    "- remove the booking device.\n",
    "\n",
    "First we merge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge DataFrames\n",
    "merged_df = pd.concat(hotelsbookings, ignore_index=True)\n",
    "# Display the merged DataFrame\n",
    "print(\"Merged DataFrame:\")\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we've seen a lot of not known or bad data. Just clean it. We want order by date, in descending mode and take only the last 2 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Arrival'] = pd.to_datetime(merged_df['Arrival'])\n",
    "# Sorting by 'Arrival' column in descending order\n",
    "sorted_bookings_df = merged_df.sort_values(by='Arrival', ascending=False)\n",
    "# Filtering for HotelId '001'\n",
    "filtered_df = sorted_bookings_df.loc[sorted_bookings_df['HotelId'] == '010']\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I convert datates to timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering for HotelId '001'\n",
    "nan_rows = sorted_bookings_df[sorted_bookings_df['Arrival'].isna()]\n",
    "print(\"Rows with NaN in 'Arrival':\", len(nan_rows))\n",
    "nan_rows = sorted_bookings_df[sorted_bookings_df['Departure'].isna()]\n",
    "print(\"Rows with NaN in 'Departure':\", len(nan_rows))\n",
    "# conversion in datetime\n",
    "sorted_bookings_df['Arrival'] = pd.to_datetime(sorted_bookings_df['Arrival'], errors='coerce', dayfirst=True)\n",
    "sorted_bookings_df['Departure'] = pd.to_datetime(sorted_bookings_df['Departure'], errors='coerce', dayfirst=True)\n",
    "sorted_bookings_df['LastModified'] = pd.to_datetime(sorted_bookings_df['LastModified'], errors='coerce', dayfirst=True)\n",
    "sorted_bookings_df['PurchaseDate'] = pd.to_datetime(sorted_bookings_df['PurchaseDate'], errors='coerce', dayfirst=True)\n",
    "# we want to make sure that are numerical data\n",
    "sorted_bookings_df['Total'] = sorted_bookings_df['Total'].str.replace(',', '.').astype(float)\n",
    "sorted_bookings_df['Total'] = pd.to_numeric(sorted_bookings_df['Total'])\n",
    "sorted_bookings_df['Nights'] = pd.to_numeric(sorted_bookings_df['Nights'])\n",
    "# add timestamp\n",
    "sorted_bookings_df['Arrival_Timestamp'] = sorted_bookings_df['Arrival'].astype('int64')\n",
    "sorted_bookings_df['Departure_Timestamp'] = sorted_bookings_df['Departure'].astype('int64')\n",
    "sorted_bookings_df['LastModified_Timestamp'] = pd.to_datetime(sorted_bookings_df['LastModified'], errors='coerce')\n",
    "sorted_bookings_df['Purchase_Timestamp'] = sorted_bookings_df['PurchaseDate'].astype('int64')\n",
    "sorted_bookings_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to add the city to he dataset the client has provided the following mapping:\n",
    "```\n",
    "hotel_to_city = {\n",
    "    '001': \"Rome, Italu\",\n",
    "    '002': \"Naples, Italy\",\n",
    "    '003': \"Florence, Italy\",\n",
    "    '004': \"Florence, Italy\",\n",
    "    '005': \"Naples, Italy\",\n",
    "    '006': \"Brindisi, Italy\",\n",
    "    '007': \"Latina, Italy\",\n",
    "    '008': \"Olbia, Sardinia, Italy\",\n",
    "    '009': \"Chamonix-Mont-Blanc, France\",\n",
    "    '010': \"Rome, Italy\",\n",
    "}\n",
    "```\n",
    "So we can have a complete a dataset to correlate in future with events, weather and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_to_city = {\n",
    "    '001': \"Rome\",\n",
    "    '002': \"Naples\",\n",
    "    '003': \"Florence\",\n",
    "    '004': \"Florence\",\n",
    "    '005': \"Naples\",\n",
    "    '006': \"Brindisi\",\n",
    "    '007': \"Latina\",\n",
    "    '008': \"Olbia\",\n",
    "    '009': \"Chamonix-Mont-Blanc\",\n",
    "    '010': \"Rome\",\n",
    "}\n",
    "\n",
    "# Function to get the city name based on HotelId\n",
    "def get_city(hotel_id):\n",
    "    return coordinate_to_city.get(hotel_id, \"Unknown\")\n",
    "\n",
    "# Add the City column based on the HotelId\n",
    "sorted_bookings_df['City'] = sorted_bookings_df['HotelId'].apply(get_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want also add the season because we know from the domain the booking changes of season and when the booking device is not known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we clean unknown\n",
    "sorted_bookings_df['BoookingDevice'].fillna('Unknown', inplace=True)\n",
    "def get_season(date):\n",
    "    month = date.month\n",
    "    if month in [12, 1, 2]:\n",
    "        return 'Winter'\n",
    "    elif month in [3, 4, 5]:\n",
    "        return 'Spring'\n",
    "    elif month in [6, 7, 8]:\n",
    "        return 'Summer'\n",
    "    elif month in [9, 10, 11]:\n",
    "        return 'Fall'\n",
    "sorted_bookings_df['Season'] = sorted_bookings_df['Arrival'].apply(get_season)\n",
    "sorted_bookings_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before going further and doing descriptive statistics we neeed to know is there are still NaN. It is ok we store for future purposes the dataset in parquet file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any row in any column is NaN\n",
    "has_nan = sorted_bookings_df.isna().any().any()\n",
    "print(f\"Does the DataFrame contain any NaN values? {has_nan}\")\n",
    "# Display rows with any NaN values\n",
    "rows_with_nan = sorted_bookings_df[sorted_bookings_df.isna().any(axis=1)]\n",
    "print(\"Rows with NaN values:\")\n",
    "rows_with_nan.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. We've seen that the booking channel can be a valid string or Unknown. The other thing that we want is to the hotel booking for a fixed period from 2024 to 2022. After this we can "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bookings_df['BookingChannel'] = sorted_bookings_df['BookingChannel'].fillna('Unknown')\n",
    "sorted_bookings_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I want to restrict the timing interval between 2020 and 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-01-01'\n",
    "end_date = '2024-02-28'\n",
    "sorted_bookings_df.head()\n",
    "datetime_columns = ['Arrival', 'Departure', 'PurchaseDate', 'LastModified']\n",
    "for col in datetime_columns:\n",
    "    sorted_bookings_df[col] = pd.to_datetime(sorted_bookings_df[col])\n",
    "hb_dataset = sorted_bookings_df[(sorted_bookings_df['Arrival'] >= start_date) & (sorted_bookings_df['Arrival'] <= end_date)] \n",
    "# Check if any row in any column is NaN\n",
    "has_nan = hb_dataset.isna().any().any()\n",
    "print(f\"Does the DataFrame contain any NaN values? {has_nan}\")\n",
    "if has_nan:\n",
    "# Display rows with any NaN values\n",
    "    rows_with_nan = hb_dataset[hb_dataset.isna().any(axis=1)]\n",
    "    print(\"Rows with NaN values:\")\n",
    "    rows_with_nan.head()\n",
    "else:\n",
    "    print('The dataset is ready some descriptivre statistics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the filtered DataFrame to a Parquet file\n",
    "parquet_file = 'filtered_data.parquet'\n",
    "hb_dataset.to_parquet(parquet_file)\n",
    "print(f\"Filtered DataFrame saved to {parquet_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Descriptive statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For numeric columns only\n",
    "descriptive_stats = hb_dataset.describe()\n",
    "print(\"\\nDescriptive Statistics for numeric columns in the filtered DataFrame:\")\n",
    "print(descriptive_stats)\n",
    "descriptive_stats_all = hb_dataset.describe(include='all')\n",
    "print(\"\\nDescriptive Statistics for all columns in the filtered DataFrame:\")\n",
    "print(descriptive_stats_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We know that:\n",
    "- We've 39375 booking in the period.\n",
    "- The average staying is 1.6 days for each booking.\n",
    "- The medium booking revenue is 370 euros.\n",
    "That's not enough, we want to know:\n",
    "- How frequent is a booking?\n",
    "- Which between our customers how had most revenue?\n",
    "- Which has most room booked and and in which city?\n",
    "- Which is the season in which we've most rooom booked? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2020-01-01'\n",
    "end_date = '2024-12-31'\n",
    "filtered_df = hb_dataset[(hb_dataset['PurchaseDate'] >= start_date) & (hb_dataset['PurchaseDate'] <= end_date)]\n",
    "\n",
    "# Plot the distribution of purchase dates using Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_df['PurchaseDate'], kde=True, bins=30)\n",
    "plt.xlabel('Purchase Date')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Purchase Dates (2020-2024)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(filtered_df['Arrival'], kde=True, bins=30)\n",
    "plt.xlabel('Arrival Date')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Arrival Dates (2020-2024)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sense. Most booking are at the beginning of the year and just before summer. Italians tends to go in vacation on August so in July the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'PurchaseDate' to ordinal for fitting distributions\n",
    "purchase_dates = hb_dataset['PurchaseDate'].apply(lambda x: x.toordinal())\n",
    "\n",
    "# List of distributions to check\n",
    "distributions = ['norm', 'expon', 'gamma', 'lognorm', 'beta']\n",
    "\n",
    "# Fit distributions and calculate KS statistic\n",
    "results = []\n",
    "for dist_name in distributions:\n",
    "    dist = getattr(stats, dist_name)\n",
    "    params = dist.fit(purchase_dates)\n",
    "    ks_stat, p_value = stats.kstest(purchase_dates, dist_name, args=params)\n",
    "    results.append((dist_name, ks_stat, p_value))\n",
    "\n",
    "# Print results\n",
    "results_df = pd.DataFrame(results, columns=['Distribution', 'KS Statistic', 'P-Value'])\n",
    "print(results_df)\n",
    "\n",
    "# Plot the best fitting distribution\n",
    "best_dist_name = results_df.sort_values('KS Statistic').iloc[0]['Distribution']\n",
    "best_dist = getattr(stats, best_dist_name)\n",
    "best_params = best_dist.fit(purchase_dates)\n",
    "\n",
    "# Plot histogram and fitted distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(purchase_dates, kde=False, bins=30, color='skyblue', stat='density')\n",
    "\n",
    "# Plot the PDF of the best fitting distribution\n",
    "x = np.linspace(min(purchase_dates), max(purchase_dates), 1000)\n",
    "pdf_fitted = best_dist.pdf(x, *best_params)\n",
    "plt.plot(x, pdf_fitted, 'r-', label=f'Best fit: {best_dist_name}')\n",
    "\n",
    "plt.xlabel('Purchase Date (ordinal)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Best Fitting Distribution for Purchase Dates')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arrival_dates = hb_dataset['Arrival'].apply(lambda x: x.toordinal())\n",
    "\n",
    "# List of distributions to check\n",
    "distributions = ['norm', 'expon', 'gamma', 'lognorm', 'beta']\n",
    "\n",
    "# Fit distributions and calculate KS statistic\n",
    "results = []\n",
    "for dist_name in distributions:\n",
    "    dist = getattr(stats, dist_name)\n",
    "    params = dist.fit(arrival_dates)\n",
    "    ks_stat, p_value = stats.kstest(arrival_dates, dist_name, args=params)\n",
    "    results.append((dist_name, ks_stat, p_value))\n",
    "\n",
    "# Print results\n",
    "results_df = pd.DataFrame(results, columns=['Distribution', 'KS Statistic', 'P-Value'])\n",
    "print(results_df)\n",
    "\n",
    "# Plot the best fitting distribution\n",
    "best_dist_name = results_df.sort_values('KS Statistic').iloc[0]['Distribution']\n",
    "best_dist = getattr(stats, best_dist_name)\n",
    "best_params = best_dist.fit(arrival_dates)\n",
    "\n",
    "# Plot histogram and fitted distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(arrival_dates, kde=False, bins=30, color='skyblue', stat='density')\n",
    "\n",
    "# Plot the PDF of the best fitting distribution\n",
    "x = np.linspace(min(arrival_dates), max(arrival_dates), 1000)\n",
    "pdf_fitted = best_dist.pdf(x, *best_params)\n",
    "plt.plot(x, pdf_fitted, 'r-', label=f'Best fit: {best_dist_name}')\n",
    "\n",
    "plt.xlabel('Arrival Date (ordinal)')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Best Fitting Distribution for Arrival Dates')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Which between our customers had most revenue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "revenue_per_hotel = hb_dataset.groupby(['HotelId', 'City'])['Total'].sum().reset_index()\n",
    "revenue_per_hotel.sort_values(['Total'], inplace=True, ascending=False)\n",
    "revenue_per_hotel.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='HotelId', y='Total', data=revenue_per_hotel)\n",
    "plt.xlabel('Hotel ID')\n",
    "plt.ylabel('Total Revenue')\n",
    "plt.title('Total Revenue per Hotel (2020-2024)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to see in the dataset the correlation between data, but for doing this and reaching the correlation matrix we need to reduce the \n",
    "features, distiguish between categorical and numerical and doing one shot encoding, removing redundant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify numerical and categorical columns\n",
    "numerical_cols = hb_dataset.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_cols = hb_dataset.select_dtypes(include=['object']).columns\n",
    "# Drop unnecessary columns\n",
    "corr_df = hb_dataset.drop(columns=['Code', 'Arrival', 'Departure', 'PurchaseDate', 'LastModified'])\n",
    "# One-hot encoding of categorical columns using pd.get_dummies\n",
    "df_encoded = pd.get_dummies(corr_df, drop_first=True)\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_encoded.corr()\n",
    "# Display the correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected some things are evident:\n",
    "- Rome is the city with higher possible revenue having an high correlation with Price.\n",
    "- Purchase Date and Arrival Date are correlated.\n",
    "Less evident is the behaviour for Season and the cities:\n",
    "- Expected behaviour that Olbia is overcrowded in Summer, since it is in Sardinia. \n",
    "For our purpose, compute demandscore is enough since we select just Arrival and treat the dataset like a time series.\n",
    "Now we will focus in model selection based on Arrival since our goal is to compute the demand score per hotel.\n",
    "There are two algorithms:\n",
    "    - Prophet\n",
    "    -  SARIMAX\n",
    "\n",
    "We'll see which is the best one for this dataset and we compute the score but first we need to detect anomalies for time series. Before we need to see if data contains anomalies and treat them. We aggregate the data weekly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the data to get weekly demand\n",
    "hb_dataset['Week'] = hb_dataset['Arrival'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weekly_demand = hb_dataset.groupby('Week').size().reset_index(name='Demand')\n",
    "weekly_demand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly detection.\n",
    "An anomaly in a time series refers to a data point or sequence of data points that significantly deviates from the expected patterns or trends typically observed in the time series data. These anomalies can appear as abrupt changes in values, an increase in NULL values, missing segments of data, or other irregular patterns that differ from normal fluctuations. In our"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection with ISOLATION FOREST\n",
    "\n",
    " -   Normalization of the time series\n",
    " -   STL decomposition of the time series to extract the residual\n",
    " -   Apply ISOLATION FOREST to the residual\n",
    " - Remove points not considered anomalous by the Isolation Forest\n",
    " - Apply DBSCAN to the points outside the confidence region to obtain clusters of points close to each other\n",
    " - Points that do not belong to any cluster are the anomalies\n",
    "\n",
    "The Isolation Forest is a completely different type of algorithm compared to ARIMA and PROPHET. \n",
    "In fact, it does not aim to find a fitting function, but directly seeks anomalous points based on the contamination parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplot statmodels numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization of time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonal-Trend decomposition using LOESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "# Apply STL decomposition\n",
    "stl = STL(df['Value'], seasonal=13)\n",
    "result = stl.fit()\n",
    "\n",
    "# Extract the residual\n",
    "residual = result.resid\n",
    "\n",
    "# Plot the decomposition results\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decomposition results\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "# Plot residual component\n",
    "plt.figure()\n",
    "plt.plot(residual)\n",
    "plt.title(\"Residual Component\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iso_forest = IsolationForest(contamination=0.05)  # Adjust contamination as needed\n",
    "df['anomaly_score'] = iso_forest.fit_predict(residual.values.reshape(-1, 1)]\n",
    "df['is_anomaly'] = df['anomaly_score'] == -1\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df.index, df['Value'], label='Time Series')\n",
    "plt.scatter(df.index[df['is_anomaly']], df['Value'][df['is_anomaly']], color='red', label='Anomalies')\n",
    "plt.title('Time Series with Anomalies Detected by Isolation Forest')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# Plot the residual with anomalies marked\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df.index, residual, label='Residual')\n",
    "plt.scatter(df.index[df['is_anomaly']], residual[df['is_anomaly']], color='red', label='Anomalies')\n",
    "plt.title('Residual with Anomalies Detected by Isolation Forest')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalization is a rescaling of the data from the original range so that all values are within the range of 0 and 1.\n",
    "Normalization can be useful, and even required in some machine learning algorithms when your time series data has input values with differing scales.It may be required for algorithms, like k-Nearest neighbors, which uses distance calculations and Linear Regression and Artificial Neural Networks that weight input values.\n",
    "Normalization requires that you know or are able to accurately estimate the minimum and maximum observable values. You may be able to estimate these values from your available data. If your time series is trending up or down, estimating these expected values may be difficult and normalization may not be the best method to use on your problem.\n",
    "\n",
    "A value is normalized as follows:\n",
    "y = (x - min) / (max - min)\n",
    "\n",
    "Where the minimum and maximum values pertain to the value x being normalized.\n",
    "\n",
    "For example, for the temperature data, we could guesstimate the min and max observable values as 30 and -10, which are greatly over and under-estimated. We can then normalize any value like 18.8 as follows:\n",
    "y = (x - min) / (max - min)\n",
    "y = (18.8 - -10) / (30 - -10)\n",
    "y = 28.8 / 40\n",
    "y = 0.72\n",
    "\n",
    "You can see that if an x value is provided that is outside the bounds of the minimum and maximum values, that the resulting value will not be in the range of 0 and 1. You could check for these observations prior to making predictions and either remove them from the dataset or limit them to the pre-defined maximum or minimum values.\n",
    "\n",
    "You can normalize your dataset using the scikit-learn object MinMaxScaler.\n",
    "\n",
    "Good practice usage with the MinMaxScaler and other rescaling techniques is as follows:\n",
    "\n",
    "    Fit the scaler using available training data. For normalization, this means the training data will be used to estimate the minimum and maximum observable values. This is done by calling the fit() function,\n",
    "    Apply the scale to training data. This means you can use the normalized data to train your model. This is done by calling the transform() function\n",
    "    Apply the scale to data going forward. This means you can prepare new data in the future on which you want to make predictions.\n",
    "\n",
    "If needed, the transform can be inverted. This is useful for converting predictions back into their original scale for reporting or plotting. This can be done by calling the inverse_transform() function.\n",
    "\n",
    "Below is an example of normalizing the Minimum Daily Temperatures dataset.\n",
    "\n",
    "The scaler requires data to be provided as a matrix of rows and columns. The loaded time series data is loaded as a Pandas Series. It must then be reshaped into a matrix of one column with 3,650 rows.\n",
    "\n",
    "The reshaped dataset is then used to fit the scaler, the dataset is normalized, then the normalization transform is inverted to show the original values again.\n",
    "# Normalize time series data\n",
    "from pandas import read_csv\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# load the dataset and print the first 5 rows\n",
    "series = read_csv('daily-minimum-temperatures-in-me.csv', header=0, index_col=0)\n",
    "print(series.head())\n",
    "# prepare data for normalization\n",
    "values = series.values\n",
    "values = values.reshape((len(values), 1))\n",
    "# train the normalization\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler = scaler.fit(values)\n",
    "print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n",
    "# normalize the dataset and print the first 5 rows\n",
    "normalized = scaler.transform(values)\n",
    "for i in range(5):\n",
    " print(normalized[i])\n",
    "# inverse transform and print the first 5 rows\n",
    "inversed = scaler.inverse_transform(normalized)\n",
    "for i in range(5):\n",
    " print(inversed[i])\n",
    "\n",
    "Running the example prints the first 5 rows from the loaded dataset, shows the same 5 values in their normalized form, then the values back in their original scale using the inverse transform.\n",
    "\n",
    "We can also see that the minimum and maximum values of the dataset are 0 and 26.3 respectively.\n",
    "Date\n",
    "1981-01-01 20.7\n",
    "1981-01-02 17.9\n",
    "1981-01-03 18.8\n",
    "1981-01-04 14.6\n",
    "1981-01-05 15.8\n",
    "Name: Temp, dtype: float64\n",
    "Min: 0.000000, Max: 26.300000\n",
    "[ 0.78707224]\n",
    "[ 0.68060837]\n",
    "[ 0.7148289]\n",
    "[ 0.55513308]\n",
    "[ 0.60076046]\n",
    "[ 20.7]\n",
    "[ 17.9]\n",
    "[ 18.8]\n",
    "[ 14.6]\n",
    "[ 15.8]\n",
    "\n",
    "There is another type of rescaling that is more robust to new values being outside the range of expected values; this is called Standardization. We will look at that next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " STL uses LOESS (locally estimated scatterplot smoothing) to extract smooths estimates of the three components. The key inputs into STL are:\n",
    "\n",
    "    season - The length of the seasonal smoother. Must be odd.\n",
    "\n",
    "    trend - The length of the trend smoother, usually around 150% of season. Must be odd and larger than season.\n",
    "\n",
    "    low_pass - The length of the low-pass estimation window, usually the smallest odd number larger than the periodicity of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Computing STL and ploting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = STL(y, period=period, seasonal=seasonal)\n",
    "%timeit mod.fit()\n",
    "res = mod.fit()\n",
    "fig = res.plot(observed=False, resid=False)\n",
    "# Extract the residual\n",
    "residual = result.resid\n",
    "\n",
    "# Plot the decomposition results\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "# Plot residual component\n",
    "plt.figure()\n",
    "plt.plot(residual)\n",
    "plt.title(\"Residual Component\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Aggregate the data to get weekly demand\n",
    "hb_dataset['Week'] = hb_dataset['Arrival'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "weekly_demand = hb_dataset.groupby('Week').size().reset_index(name='Demand')\n",
    "weekly_demand.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare the data for Prophet\n",
    "weekly_demand_prophet = weekly_demand.rename(columns={'Week': 'ds', 'Demand': 'y'})\n",
    "\n",
    "# Fit Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_model.fit(weekly_demand_prophet)\n",
    "\n",
    "# Make a future dataframe for Prophet\n",
    "future_prophet = prophet_model.make_future_dataframe(periods=52, freq='W')\n",
    "forecast_prophet = prophet_model.predict(future_prophet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_demand_prophet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_prophet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit ARIMA model\n",
    "arima_model = SARIMAX(weekly_demand['Demand'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 52))\n",
    "arima_model_fit = arima_model.fit(disp=False)\n",
    "# Forecast using ARIMA\n",
    "forecast_arima = arima_model_fit.get_forecast(steps=52)\n",
    "forecast_arima_df = forecast_arima.summary_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the forecast data for comparison\n",
    "forecast_prophet = forecast_prophet[['ds', 'yhat']].set_index('ds')\n",
    "forecast_arima_df = forecast_arima_df[['mean']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance\n",
    "Better SARIMA. So we will use SARIMA for the compute of the score based on arrival. Let's motivate the process:\n",
    "- Mean Average Error (MSE): Tells us the average magnitude of the forecast errors. \n",
    "- Mean Square Error: Emphasizes larger errors more than MAE. \n",
    "- Root Mean Square Error: Provides an error measure in the same units as the demand. \n",
    "\n",
    "## Comparing Models\n",
    "- SARIMA shows an higher MAE than Prophet but perform better since MSE and RMSE are lower.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute evaluation metrics\n",
    "# For Prophet\n",
    "prophet_true = weekly_demand['Demand'].values[-52:]\n",
    "prophet_pred = forecast_prophet['yhat'].values[:52]\n",
    "prophet_mae = mean_absolute_error(prophet_true, prophet_pred)\n",
    "prophet_mse = mean_squared_error(prophet_true, prophet_pred)\n",
    "prophet_rmse = np.sqrt(prophet_mse)\n",
    "\n",
    "# For ARIMA\n",
    "arima_true = weekly_demand['Demand'].values[-52:]\n",
    "arima_pred = forecast_arima_df['mean'].values[:52]\n",
    "arima_mae = mean_absolute_error(arima_true, arima_pred)\n",
    "arima_mse = mean_squared_error(arima_true, arima_pred)\n",
    "arima_rmse = np.sqrt(arima_mse)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"Prophet MAE: {prophet_mae}, MSE: {prophet_mse}, RMSE: {prophet_rmse}\")\n",
    "print(f\"ARIMA MAE: {arima_mae}, MSE: {arima_mse}, RMSE: {arima_rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Review Score</b> using BERT Sentiment Analysis over Tripadvisor Reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Hotel performance in terms of revenue depends stricly from online reputation (cit), so our goal in designing <b>HotelRank</p> algorithm is to analyze reviews about our customers hotel and provide an unique score to put a linear factor in HotelRank. We can name that score as <i>review score</i>.</p>\n",
    "<p><B>ReviewScore = Hotel_Rating - (0.5 * #number_negative review) + (0.25 * number of positive review)</B></p>\n",
    "<p>Here we emphasize the idea that is better not having negitive than positive reviews. To compute this formula we need we are in need to perform sentiment analysis over hotel reviews.</p><p>In case the <b>ReviewScore</b> is negative we assume that his weight to HotelRank is 0</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The state-of-the-art sentiment analysis can be accomplished by fine-tuning pretrained BERT models with sentiment-analysis datasets. Fine-tuning is accomplished by further training a pretrained model for a limited number of epochs and with a reduced learning rate. We have collected the review using a scraper that we've coded in Go programming Language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize(samples):\n",
    "    return tokenizer(samples['Text'], truncation=True)\n",
    "\n",
    "tokenized_imdb = imdb.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the reviews are tokenized, they need to be converted from [Hugging Face datasets](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset) into [TensorFlow datasets](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) with Hugging Face’s [Dataset.to_tf_dataset](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset) method. The collating function passed to that method dynamically pads the sequences so they’re all the same length. You can also ask the tokenizer to do the padding, but padding performed that way is static and requires more memory:\n",
    "                                                                                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors='tf')\n",
    "\n",
    "train_data = tokenized_imdb['train'].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'label'],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "validation_data = tokenized_imdb['test'].to_tf_dataset(\n",
    "    columns=['attention_mask', 'input_ids', 'label'],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you’re ready to fine-tune. Call `fit` on the model as usual, but set the `Adam` optimizer’s learning rate (the nominal amount that weights and biases are adjusted during backpropagation passes) to 0.00002, which is a fraction of the default learning rate of 0.001:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)\n",
    "model.compile(Adam(learning_rate=2e-5), metrics=['accuracy'])\n",
    "hist = model.fit(train_data, validation_data=validation_data, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "acc = hist.history['accuracy']\n",
    "val = hist.history['val_accuracy']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, '-', label='Training accuracy')\n",
    "plt.plot(epochs, val, ':', label='Validation accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finish up by defining an `analyze_text` function that returns a sentiment score and using it to score a positive review for sentiment. The model returns an object wrapping a tensor containing unnormalized sentiment scores (negative and positive), but you can use TensorFlow’s `softmax` function to normalize them to values from 0.0 to 1.0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import panda as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "def analyze_text(text, tokenizer, model):\n",
    "    tokenized_text = tokenizer(text, padding=True, truncation=True, return_tensors='tf')\n",
    "    prediction = model(tokenized_text)\n",
    "    return tf.nn.softmax(prediction[0]).numpy()[0][1]\n",
    "\n",
    "review_data = glob.glob('review_data/*.csv')\n",
    "datasets = [pd.read_csv(filedata) for filedata in review_data]\n",
    "datasets[0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancellation Score: Analysis on Cancellations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
